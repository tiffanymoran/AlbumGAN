{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP-training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMcTVfURUhtB2rC4IZGCztm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiffanymoran/AlbumGAN/blob/main/CLIP_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYnWlWB6Pwr7",
        "outputId": "11bab1e8-94ca-4779-b6aa-e8c7c58a1a0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-3zzjidnz\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-3zzjidnz\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.0+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.0+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import optim\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBOQJ4z4Py7v",
        "outputId": "a5be9117-8665-45da-db0d-fd260a574896"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 1.12.0+cu113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4UhDZ7wP28i",
        "outputId": "3b84f8be-44fb-490f-9e4e-88f28a957c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['RN50',\n",
              " 'RN101',\n",
              " 'RN50x4',\n",
              " 'RN50x16',\n",
              " 'RN50x64',\n",
              " 'ViT-B/32',\n",
              " 'ViT-B/16',\n",
              " 'ViT-L/14',\n",
              " 'ViT-L/14@336px']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/CLIP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tfm5Yn2LRnHJ",
        "outputId": "178791d8-0ffb-402d-a267-5c7128b1998d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CLIP' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "BdI8wFB7iGsU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b27e14-fae0-4a7d-9e62-65fb789408a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\" # If using GPU then use mixed precision training.\n",
        "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training"
      ],
      "metadata": {
        "id": "QvMHLGWVP8qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restructure CSV for CLIP training"
      ],
      "metadata": {
        "id": "gzti8qSR2DzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/ColabNotebooks/Images/captionsfinal.csv',\n",
        "                 names=[\"Path\", \"Drop\", \"Caption\"])"
      ],
      "metadata": {
        "id": "lwefbzD9pvdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Caption'] = df['Caption'].str[:255] #truncate lyrics to make sure they aren't too long"
      ],
      "metadata": {
        "id": "B4mmLWaex1FH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169
        },
        "outputId": "a0fb5399-1020-4545-fad4-3c7d842429b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b824a39ab217>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Caption'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Caption'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#truncate lyrics to make sure they aren't too long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Path'] = '/content/drive/MyDrive/ColabNotebooks/Images/' + df['Path'].astype(str) #add the path to the file name"
      ],
      "metadata": {
        "id": "POMw1cuOJVvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_list = df.Drop.values.tolist() #create lists for training script\n",
        "caption_list = df.Caption.values.tolist()"
      ],
      "metadata": {
        "id": "qIYk8MyPscSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "g1VQSXoe2kIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class image_title_dataset(Dataset):\n",
        "    def __init__(self, list_image_path,list_txt):\n",
        "\n",
        "        self.image_path = list_image_path\n",
        "        self.title  = clip.tokenize(list_txt) #you can tokenize everything at once in here(slow at the beginning), or tokenize it in the training loop.\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.title)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = preprocess(Image.open(self.image_path[idx])) # Image from PIL module\n",
        "        title = self.title[idx]\n",
        "        return image,title"
      ],
      "metadata": {
        "id": "hKrWTbFRQLIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use your own data\n",
        "list_image_path = image_list \n",
        "list_txt = caption_list\n",
        "dataset = image_title_dataset(list_image_path,list_txt)\n",
        "train_dataloader = DataLoader(dataset,batch_size = 32) #Define your own dataloader"
      ],
      "metadata": {
        "id": "baBEtnPFQO_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://github.com/openai/CLIP/issues/57\n",
        "def convert_models_to_fp32(model): \n",
        "    for p in model.parameters(): \n",
        "        p.data = p.data.float() \n",
        "        p.grad.data = p.grad.data.float() \n",
        "\n",
        "if device == \"cpu\":\n",
        "  model.float()\n",
        "else :\n",
        "  clip.model.convert_weights(model) # Actually this line is unnecessary since clip by default already on float16\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_txt = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-5,betas=(0.9,0.98),eps=1e-6,weight_decay=0.2) #Params used from paper, the lr is smaller, more safe for fine tuning to new dataset\n",
        "\n",
        "EPOCH = 10\n",
        "\n",
        "# add your own code to track the training progress.\n",
        "for epoch in range(EPOCH):\n",
        "  for batch in train_dataloader :\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      images,texts = batch \n",
        "    \n",
        "      images= images.to(device)\n",
        "      texts = texts.to(device)\n",
        "    \n",
        "      logits_per_image, logits_per_text = model(images, texts)\n",
        "\n",
        "      ground_truth = torch.arange(len(images),dtype=torch.long,device=device)\n",
        "\n",
        "      total_loss = (loss_img(logits_per_image,ground_truth) + loss_txt(logits_per_text,ground_truth))/2\n",
        "      total_loss.backward()\n",
        "      if device == \"cpu\":\n",
        "         optimizer.step()\n",
        "      else : \n",
        "        convert_models_to_fp32(model)\n",
        "        optimizer.step()\n",
        "        clip.model.convert_weights(model)"
      ],
      "metadata": {
        "id": "weM3yTjcQXJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(total_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5Q6BYvL-uF7",
        "outputId": "717da2f5-f31e-48b9-a0ed-00a159e9363f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(2.5664, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Model"
      ],
      "metadata": {
        "id": "11_eK_uF2ZEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': total_loss,\n",
        "        }, f\"/content/drive/MyDrive/ColabNotebooks/model_10.pt\") #can update with preferred folder / file name"
      ],
      "metadata": {
        "id": "6qVhcOAOQaEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reload Model"
      ],
      "metadata": {
        "id": "B0jwebRV2cNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\",device=device,jit=False) #Must set jit=False for training\n",
        "checkpoint = torch.load(\"/content/drive/MyDrive/ColabNotebooks/model_10.pt\")"
      ],
      "metadata": {
        "id": "Zgcloca9Qa6W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7LRw0qwQq6j",
        "outputId": "aa69e921-7015-4e6d-9462-056dc017d7f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsGYSalbQcC4",
        "outputId": "4d9324d6-6b0e-4f63-f868-50e5e81cf0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's state_dict:\n",
            "positional_embedding \t torch.Size([77, 512])\n",
            "text_projection \t torch.Size([512, 512])\n",
            "logit_scale \t torch.Size([])\n",
            "visual.class_embedding \t torch.Size([768])\n",
            "visual.positional_embedding \t torch.Size([50, 768])\n",
            "visual.proj \t torch.Size([768, 512])\n",
            "visual.conv1.weight \t torch.Size([768, 3, 32, 32])\n",
            "visual.ln_pre.weight \t torch.Size([768])\n",
            "visual.ln_pre.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.0.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.0.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.0.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.0.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.0.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.0.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.0.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.0.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.0.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.0.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.0.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.0.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.1.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.1.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.1.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.1.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.1.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.1.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.1.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.1.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.1.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.1.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.1.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.1.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.2.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.2.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.2.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.2.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.2.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.2.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.2.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.2.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.2.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.2.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.2.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.2.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.3.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.3.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.3.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.3.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.3.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.3.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.3.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.3.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.3.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.3.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.3.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.3.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.4.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.4.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.4.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.4.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.4.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.4.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.4.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.4.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.4.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.4.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.4.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.4.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.5.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.5.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.5.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.5.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.5.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.5.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.5.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.5.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.5.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.5.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.5.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.5.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.6.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.6.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.6.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.6.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.6.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.6.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.6.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.6.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.6.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.6.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.6.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.6.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.7.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.7.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.7.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.7.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.7.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.7.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.7.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.7.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.7.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.7.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.7.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.7.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.8.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.8.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.8.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.8.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.8.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.8.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.8.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.8.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.8.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.8.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.8.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.8.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.9.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.9.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.9.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.9.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.9.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.9.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.9.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.9.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.9.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.9.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.9.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.9.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.10.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.10.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.10.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.10.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.10.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.10.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.10.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.10.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.10.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.10.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.10.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.10.ln_2.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.11.attn.in_proj_weight \t torch.Size([2304, 768])\n",
            "visual.transformer.resblocks.11.attn.in_proj_bias \t torch.Size([2304])\n",
            "visual.transformer.resblocks.11.attn.out_proj.weight \t torch.Size([768, 768])\n",
            "visual.transformer.resblocks.11.attn.out_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.11.ln_1.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.11.ln_1.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.11.mlp.c_fc.weight \t torch.Size([3072, 768])\n",
            "visual.transformer.resblocks.11.mlp.c_fc.bias \t torch.Size([3072])\n",
            "visual.transformer.resblocks.11.mlp.c_proj.weight \t torch.Size([768, 3072])\n",
            "visual.transformer.resblocks.11.mlp.c_proj.bias \t torch.Size([768])\n",
            "visual.transformer.resblocks.11.ln_2.weight \t torch.Size([768])\n",
            "visual.transformer.resblocks.11.ln_2.bias \t torch.Size([768])\n",
            "visual.ln_post.weight \t torch.Size([768])\n",
            "visual.ln_post.bias \t torch.Size([768])\n",
            "transformer.resblocks.0.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.0.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.0.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.0.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.0.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.0.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.0.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.0.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.0.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.0.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.0.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.0.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.1.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.1.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.1.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.1.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.1.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.1.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.1.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.1.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.1.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.1.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.1.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.1.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.2.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.2.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.2.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.2.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.2.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.2.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.2.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.2.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.2.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.2.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.2.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.2.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.3.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.3.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.3.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.3.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.3.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.3.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.3.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.3.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.3.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.3.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.3.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.3.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.4.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.4.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.4.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.4.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.4.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.4.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.4.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.4.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.4.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.4.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.4.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.4.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.5.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.5.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.5.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.5.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.5.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.5.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.5.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.5.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.5.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.5.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.5.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.5.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.6.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.6.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.6.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.6.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.6.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.6.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.6.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.6.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.6.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.6.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.6.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.6.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.7.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.7.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.7.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.7.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.7.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.7.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.7.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.7.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.7.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.7.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.7.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.7.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.8.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.8.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.8.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.8.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.8.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.8.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.8.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.8.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.8.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.8.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.8.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.8.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.9.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.9.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.9.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.9.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.9.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.9.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.9.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.9.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.9.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.9.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.9.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.9.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.10.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.10.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.10.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.10.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.10.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.10.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.10.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.10.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.10.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.10.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.10.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.10.ln_2.bias \t torch.Size([512])\n",
            "transformer.resblocks.11.attn.in_proj_weight \t torch.Size([1536, 512])\n",
            "transformer.resblocks.11.attn.in_proj_bias \t torch.Size([1536])\n",
            "transformer.resblocks.11.attn.out_proj.weight \t torch.Size([512, 512])\n",
            "transformer.resblocks.11.attn.out_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.11.ln_1.weight \t torch.Size([512])\n",
            "transformer.resblocks.11.ln_1.bias \t torch.Size([512])\n",
            "transformer.resblocks.11.mlp.c_fc.weight \t torch.Size([2048, 512])\n",
            "transformer.resblocks.11.mlp.c_fc.bias \t torch.Size([2048])\n",
            "transformer.resblocks.11.mlp.c_proj.weight \t torch.Size([512, 2048])\n",
            "transformer.resblocks.11.mlp.c_proj.bias \t torch.Size([512])\n",
            "transformer.resblocks.11.ln_2.weight \t torch.Size([512])\n",
            "transformer.resblocks.11.ln_2.bias \t torch.Size([512])\n",
            "token_embedding.weight \t torch.Size([49408, 512])\n",
            "ln_final.weight \t torch.Size([512])\n",
            "ln_final.bias \t torch.Size([512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQg1A-HVRZ2y",
        "outputId": "8a7c4ed0-699a-451c-f01e-6641546688a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): Sequential(\n",
              "      (0): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (6): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (7): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (8): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (9): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (10): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (11): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (token_embedding): Embedding(49408, 512)\n",
              "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v0S98GOoRayf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}